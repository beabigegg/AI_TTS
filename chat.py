# scripts/chat.py
# (åŸ 07_interactive_chat.pyï¼Œå·²ä½¿ç”¨ torch.hub è¼‰å…¥ WaveGlow ä¸¦ä¿®æ­£ sample_rate)
# ç”¨æ³•: python -m scripts.chat [--model_path "path/to/your_model.pth"] [--llm_model "qwen:7b"]

import torch
import torchaudio
from pathlib import Path
import argparse
import ollama
import json
import simpleaudio as sa
import tempfile
import sys
import os

# ä½¿ç”¨ç›¸å°å°å…¥
from .common import TextToMelModel, text_to_sequence, _symbols

# --- TTS åˆæˆå‡½å¼ ---
def synthesize_speech_for_chat(text: str, model: TextToMelModel, vocoder, device: str, sample_rate: int) -> str | None:
    """
    æ¥æ”¶æ–‡æœ¬ï¼Œç”ŸæˆèªéŸ³ï¼Œä¸¦å°‡å…¶å„²å­˜ç‚ºä¸€å€‹æš«å­˜ .wav æª”æ¡ˆã€‚
    è¿”å›è©²æš«å­˜æª”æ¡ˆçš„è·¯å¾‘ï¼Œå¦‚æœåˆæˆå¤±æ•—å‰‡è¿”å› Noneã€‚
    """
    if not text.strip():
        print("[TTS è­¦å‘Š] è¼¸å…¥æ–‡æœ¬ç‚ºç©ºï¼Œè·³éåˆæˆã€‚")
        return None

    sequence_list = text_to_sequence(text)
    if not sequence_list:
        print(f"[TTS è­¦å‘Š] æ–‡æœ¬ '{text[:30]}...' è½‰æ›ç‚ºéŸ³ç´ åºåˆ—å¾Œç‚ºç©ºï¼Œè·³éåˆæˆã€‚")
        return None
    sequence = torch.LongTensor(sequence_list).unsqueeze(0).to(device)
    
    with torch.no_grad():
        try:
            mel_prediction = model(sequence)
            waveform, *_ = vocoder.infer(mel_prediction) # WaveGlow
        except Exception as e:
            print(f"[TTS éŒ¯èª¤] ç”Ÿæˆæ¢…çˆ¾é »è­œæˆ–æ³¢å½¢æ™‚å¤±æ•—: {e}")
            return None

    try:
        temp_wav_path = tempfile.mktemp(suffix=".wav", prefix="chat_audio_", dir=Path("outputs")/ "temp_chat_audio")
        Path(temp_wav_path).parent.mkdir(parents=True, exist_ok=True) # ç¢ºä¿æš«å­˜ç›®éŒ„å­˜åœ¨
        torchaudio.save(temp_wav_path, waveform.cpu(), sample_rate=sample_rate)
        return temp_wav_path
    except Exception as e:
        print(f"[TTS éŒ¯èª¤] å„²å­˜æš«å­˜éŸ³è¨Šæª”æ™‚å¤±æ•—: {e}")
        return None

# --- ä¸»èŠå¤©å¾ªç’° ---
def main_chat_loop(args):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"å°‡ä½¿ç”¨ {device} è£ç½®é‹è¡Œ TTS æ¨¡å‹ã€‚")

    # 1. è¼‰å…¥ TTS æ¨¡å‹å’Œè²ç¢¼å™¨
    print("æ­£åœ¨è¼‰å…¥ TTS æ¨¡å‹èˆ‡ WaveGlow è²ç¢¼å™¨...")
    if not args.model_path.exists():
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° TTS æ¨¡å‹æª”æ¡ˆ {args.model_path}")
        sys.exit(1)
    try:
        tts_model = TextToMelModel(n_vocab=len(_symbols), n_mels=80)
        tts_model.load_state_dict(torch.load(args.model_path, map_location=device))
        tts_model.to(device).eval()

        vocoder = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp16', pretrained=True, progress=True)
        vocoder = vocoder.remove_weightnorm(vocoder)
        vocoder = vocoder.to(device).eval()
        vocoder_sample_rate = 22050 # WaveGlow é è¨­
    except Exception as e:
        print(f"è¼‰å…¥æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        sys.exit(1)
    
    print(f"TTS å¼•æ“å·²æº–å‚™å°±ç·’ï¼(æ¡æ¨£ç‡: {vocoder_sample_rate}Hz)")
    print("-" * 50)

    # 2. è¨­å®š LLM
    system_prompt = """
    ä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„AIåŠ©ç†ã€‚
    è«‹åš´æ ¼ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼å›è¦†ï¼Œä¸åŒ…å«ä»»ä½•é¡å¤–çš„è§£é‡‹æˆ–æ–‡å­—ï¼š
    {
      "japanese": "é€™è£¡æ˜¯ç”¨æ–¼èªéŸ³è¼¸å‡ºçš„æ—¥æ–‡å›ç­”",
      "chinese": "é€™è£¡æ˜¯å°æ‡‰çš„ä¸­æ–‡ç¿»è­¯ï¼Œç”¨æ–¼å­—å¹•"
    }
    å¦‚æœä½¿ç”¨è€…çš„å•é¡Œä¸é©åˆç”¨æ—¥æ–‡å›ç­”ï¼Œæˆ–è€…ä½ ç„¡æ³•ç”Ÿæˆæœ‰æ„ç¾©çš„æ—¥æ–‡å›ç­”ï¼Œ
    è«‹åœ¨ "japanese" æ¬„ä½ä¸­å¡«å¯«é¡ä¼¼ "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€æ—¥æœ¬èªã§ã®å›ç­”ã¯é›£ã—ã„ã§ã™ã€‚" çš„å¥å­ã€‚
    """
    messages = [{'role': 'system', 'content': system_prompt}]

    print(f"æ­¡è¿ä¾†åˆ°äº’å‹•å¼èŠå¤© (LLM: {args.llm_model})ã€‚è¼¸å…¥ 'exit' æˆ– 'quit' ä¾†çµæŸå°è©±ã€‚")

    temp_audio_dir = Path("outputs") / "temp_chat_audio"
    temp_audio_dir.mkdir(parents=True, exist_ok=True)


    while True:
        try:
            user_input = input("æ‚¨: ")
            if user_input.lower() in ['exit', 'quit', 'å†è¦‹', 'ã•ã‚ˆã†ãªã‚‰']:
                print("æ„Ÿè¬ä½¿ç”¨ï¼Œå†è¦‹ï¼")
                break
            if not user_input.strip():
                continue

            messages.append({'role': 'user', 'content': user_input})
            
            print("\nAI æ€è€ƒä¸­...")
            try:
                response = ollama.chat(model=args.llm_model, messages=messages, options={"temperature": 0.7})
                ai_response_content = response['message']['content']
                messages.append({'role': 'assistant', 'content': ai_response_content})
            except Exception as e:
                print(f"\n[LLM éŒ¯èª¤] èˆ‡ Ollama æ¨¡å‹é€šè¨Šå¤±æ•—: {e}")
                messages.pop() # ç§»é™¤å¤±æ•—çš„ä½¿ç”¨è€…è¼¸å…¥ï¼Œä»¥å…å½±éŸ¿ä¸‹æ¬¡å°è©±
                continue


            try:
                response_data = json.loads(ai_response_content)
                jp_text = response_data.get("japanese")
                cn_text = response_data.get("chinese")

                if not isinstance(jp_text, str) or not isinstance(cn_text, str): # ç¢ºä¿æ˜¯å­—ä¸²
                    raise ValueError("JSON å›è¦†ä¸­çš„ 'japanese' æˆ– 'chinese' éµä¸æ˜¯æœ‰æ•ˆçš„å­—ä¸²ã€‚")

                print("-" * 50)
                print(f"å­—å¹• (ä¸­æ–‡): {cn_text}")
                print("AI (æ—¥æ–‡): " + jp_text)
                
                if jp_text.strip(): # åªæœ‰ç•¶æ—¥æ–‡æ–‡æœ¬éç©ºæ™‚æ‰åˆæˆå’Œæ’­æ”¾
                    print("\nğŸ”Š æ­£åœ¨ç”ŸæˆèªéŸ³...")
                    wav_path = synthesize_speech_for_chat(jp_text, tts_model, vocoder, device, vocoder_sample_rate)
                    
                    if wav_path:
                        print("â–¶ï¸ æ­£åœ¨æ’­æ”¾è²éŸ³...")
                        try:
                            wave_obj = sa.WaveObject.from_wave_file(wav_path)
                            play_obj = wave_obj.play()
                            play_obj.wait_done()
                        except Exception as e_play:
                            print(f"[æ’­æ”¾éŒ¯èª¤] æ’­æ”¾éŸ³è¨Š {wav_path} å¤±æ•—: {e_play}")
                        finally:
                            try: # å˜—è©¦åˆªé™¤æš«å­˜æª”æ¡ˆ
                                os.remove(wav_path)
                            except OSError:
                                pass # å¦‚æœåˆªé™¤å¤±æ•—ï¼Œå¿½ç•¥
                    else:
                        print("[TTS] æœªèƒ½ç”Ÿæˆæœ‰æ•ˆèªéŸ³ã€‚")
                else:
                    print("[TTS] æ—¥æ–‡æ–‡æœ¬ç‚ºç©ºï¼Œä¸é€²è¡ŒèªéŸ³åˆæˆã€‚")
                print("-" * 50)

            except (json.JSONDecodeError, ValueError) as e:
                print(f"\n[è§£æéŒ¯èª¤] ç„¡æ³•è§£æ AI çš„å›è¦†æˆ–å›è¦†æ ¼å¼ä¸æ­£ç¢º: {e}")
                print(f"åŸå§‹å›è¦†å…§å®¹: {ai_response_content}")
                print("-" * 50)

        except KeyboardInterrupt:
            print("\nåµæ¸¬åˆ°ä¸­æ–·æŒ‡ä»¤ï¼Œå†è¦‹ï¼")
            break
        except Exception as e:
            print(f"\nç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")
            break
        finally:
            # æ¸…ç†å¯èƒ½æ®˜ç•™çš„æš«å­˜éŸ³è¨Šæª”æ¡ˆ
            for f_temp in temp_audio_dir.glob("chat_audio_*.wav"):
                try:
                    os.remove(f_temp)
                except OSError:
                    pass


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="èˆ‡æœ¬åœ° LLM å’Œ TTS æ¨¡å‹é€²è¡Œäº’å‹•å¼èŠå¤©")
    parser.add_argument("--model_path", type=Path, default=Path("outputs/models/text_to_mel_model.pth"), 
                        help="è¨“ç·´å¥½çš„ TextToMel æ¨¡å‹çš„è·¯å¾‘")
    parser.add_argument("--llm_model", type=str, default="qwen3:8b", 
                        help="è¦ä½¿ç”¨çš„ Ollama æ¨¡å‹åç¨± (ä¾‹å¦‚ qwen3:8b, llama3, etc.)")
    
    args = parser.parse_args()
    main_chat_loop(args)